@INPROCEEDINGS{Vestias,
  author={Véstias, Mário and Neto, Horácio},
  booktitle={2014 24th International Conference on Field Programmable Logic and Applications (FPL)}, 
  title={Trends of CPU, GPU and FPGA for high-performance computing}, 
  year={2014},
  volume={},
  number={},
  pages={1-6},
  keywords={Field programmable gate arrays;Graphics processing units;Performance evaluation;Market research;Table lookup;Monte Carlo methods},
  doi={10.1109/FPL.2014.6927483}}

@book{Eijkhout,
author = {Eijkhout, Victor and van de Geijn, Robert and Chow, Edmond},
year = {2016},
month = {01},
title = {Introduction to High Performance Scientific Computing},
doi = {10.5281/zenodo.49897}
}

@article{Gamba,
title={Parall{\'e}lisation d'un outil de simulation hydraulique num{\'e}rique},
author={Gamba, Emilio and Macq, Jean-Baptiste},
year={2018}
}

@standard{ieee754-2019,
  title        = {IEEE Standard for Floating-Point Arithmetic},
  organization = {IEEE},
  year         = {2019},
  number       = {IEEE Std 754-2019 (Revision of IEEE 754-2008)},
  doi          = {10.1109/IEEESTD.2019.8766229},
  url          = {https://doi.org/10.1109/IEEESTD.2019.8766229},
}


@article{CUDA,
author = {Nickolls, John and Buck, Ian and Garland, Michael and Skadron, Kevin},
year = {2008},
month = {03},
pages = {40-53},
title = {Scalable Parallel Programming with CUDA},
volume = {6},
journal = {Queue},
doi = {10.1145/1401132.1401152}
}

@InProceedings{OpenACC,
author="Wienke, Sandra
and Springer, Paul
and Terboven, Christian
and an Mey, Dieter",
editor="Kaklamanis, Christos
and Papatheodorou, Theodore
and Spirakis, Paul G.",
title="OpenACC --- First Experiences with Real-World Applications",
booktitle="Euro-Par 2012 Parallel Processing",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="859--870",
abstract="Today's trend to use accelerators like GPGPUs in heterogeneous computer systems has entailed several low-level APIs for accelerator programming. However, programming these APIs is often tedious and therefore unproductive. To tackle this problem, recent approaches employ directive-based high-level programming for accelerators. In this work, we present our first experiences with OpenACC, an API consisting of compiler directives to offload loops and regions of C/C++ and Fortran code to accelerators. We compare the performance of OpenACC to PGI Accelerator and OpenCL for two real-world applications and evaluate programmability and productivity. We find that OpenACC offers a promising ratio of development effort to performance and that a directive-based approach to program accelerators is more efficient than low-level APIs, even if suboptimal performance is achieved.",
isbn="978-3-642-32820-6"
}

@misc{HIP,
  author = {Advanced Micro Devices (AMD)},
  title = {Heterogeneous-Compute Interface for Portability (HIP)},
  year = {2024},
  howpublished = {ROCm Documentation},
  url = {https://rocm.docs.amd.com}
}

@INPROCEEDINGS{Xu2016,
  author={Xu, Jingheng and Fu, Haohuan and Gan, Lin and Yang, Chao and Xue, Wei and Xu, Shizhen and Zhao, Wenlai and Wang, Xinliang and Chen, Bingwei and Yang, Guangwen},
  booktitle={2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)}, 
  title={Generalized GPU Acceleration for Applications Employing Finite-Volume Methods}, 
  year={2016},
  volume={},
  number={},
  pages={126-135},
  keywords={Graphics processing units;Instruction sets;Computer architecture;Mathematical model;Atmospheric modeling;Optimization methods;high performance computing;GPU;atmospheric simulation;FVM optimizations;Tesla K80},
  doi={10.1109/CCGrid.2016.30}}


@article{OpenCL,
author = {Stone, J. E. and Gohara, D. and Shi, Guochun},
title = {OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems},
year = {2010},
issue_date = {May 2010},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {12},
number = {3},
issn = {1521-9615},
abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
journal = {Computing in Science and Engg.},
month = may,
pages = {66–73},
numpages = {8}
}

@article{Martineau,
author = {Martineau, Matthew and McIntosh-Smith, Simon and Gaudin, Wayne},
year = {2017},
month = {03},
pages = {e4117},
title = {Assessing the performance portability of modern parallel programming models using TeaLeaf},
volume = {29},
journal = {Concurrency and Computation: Practice and Experience},
doi = {10.1002/cpe.4117}
}

@inproceedings{Henriksen,
author = {Henriksen, Troels},
title = {A Comparison of OpenCL, CUDA, and HIP as Compilation Targets for a Functional Array Language},
year = {2024},
isbn = {9798400711008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3677997.3678226},
doi = {10.1145/3677997.3678226},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Functional Programming for Productivity and Performance},
pages = {1–9},
numpages = {9},
keywords = {GPU, functional programming, parallel programming, performance measurement},
location = {Milan, Italy},
series = {FProPer 2024}
}

@INPROCEEDINGS{Usha,
  author={Usha, R. and Pandey, Prachi and Mangala, N.},
  booktitle={2020 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={A Comprehensive Comparison and Analysis of OpenACC and OpenMP 4.5 for NVIDIA GPUs}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Parallel programming;Conferences;Graphics processing units;Data transfer;Complexity theory;Kernel;HPC;OpenACC;OpenMP 4.5;NVIDIA Tesla GPu;comparison},
  doi={10.1109/HPEC43674.2020.9286203}}

  @article{Czarnul,
author = {Czarnul, Paweł and Proficz, Jerzy and Drypczewski, Krzysztof},
title = {Survey of Methodologies, Approaches, and Challenges in Parallel Programming Using High-Performance Computing Systems},
journal = {Scientific Programming},
volume = {2020},
number = {1},
pages = {4176794},
doi = {https://doi.org/10.1155/2020/4176794},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2020/4176794},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2020/4176794},
abstract = {This paper provides a review of contemporary methodologies and APIs for parallel programming, with representative technologies selected in terms of target system type (shared memory, distributed, and hybrid), communication patterns (one-sided and two-sided), and programming abstraction level. We analyze representatives in terms of many aspects including programming model, languages, supported platforms, license, optimization goals, ease of programming, debugging, deployment, portability, level of parallelism, constructs enabling parallelism and synchronization, features introduced in recent versions indicating trends, support for hybridity in parallel execution, and disadvantages. Such detailed analysis has led us to the identification of trends in high-performance computing and of the challenges to be addressed in the near future. It can help to shape future versions of programming standards, select technologies best matching programmers’ needs, and avoid potential difficulties while using high-performance computing systems.},
year = {2020}
}

@article{Wang,
  title={Research on the Competitive Development and Prospects of Nvidia},
  author={Jinchuan Wang},
  journal={Advances in Economics, Management and Political Sciences},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:275503189}
}

@InProceedings{LUMI,
author="Markomanolis, George S.
and Alpay, Aksel
and Young, Jeffrey
and Klemm, Michael
and Malaya, Nicholas
and Esposito, Aniello
and Heikonen, Jussi
and Bastrakov, Sergei
and Debus, Alexander
and Kluge, Thomas
and Steiniger, Klaus
and Stephan, Jan
and Widera, Rene
and Bussmann, Michael",
editor="Panda, Dhabaleswar K.
and Sullivan, Michael",
title="Evaluating GPU Programming Models for the LUMI Supercomputer",
booktitle="Supercomputing Frontiers",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="79--101",
abstract="It is common in the HPC community that the achieved performance with just CPUs is limited for many computational cases. The EuroHPC pre-exascale and the coming exascale systems are mainly focused on accelerators, and some of the largest upcoming supercomputers such as LUMI and Frontier will be powered by AMD Instinct{\texttrademark} accelerators. However, these new systems create many challenges for developers who are not familiar with the new ecosystem or with the required programming models that can be used to program for heterogeneous architectures. In this paper, we present some of the more well-known programming models to program for current and future GPU systems. We then measure the performance of each approach using a benchmark and a mini-app, test with various compilers, and tune the codes where necessary. Finally, we compare the performance, where possible, between the NVIDIA Volta (V100), Ampere (A100) GPUs, and the AMD MI100 GPU.",
isbn="978-3-031-10419-0"
}

@article{Kokkos,
  author={Trott, Christian R. and Lebrun-Grandié, Damien and Arndt, Daniel and Ciesko, Jan and Dang, Vinh and Ellingwood, Nathan and Gayatri, Rahulkumar and Harvey, Evan and Hollman, Daisy S. and Ibanez, Dan and Liber, Nevin and Madsen, Jonathan and Miles, Jeff and Poliakoff, David and Powell, Amy and Rajamanickam, Sivasankaran and Simberg, Mikael and Sunderland, Dan and Turcksin, Bruno and Wilke, Jeremiah},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  title={Kokkos 3: Programming Model Extensions for the Exascale Era},
  year={2022},
  volume={33},
  number={4},
  pages={805-817},
  doi={10.1109/TPDS.2021.3097283}}

  @inproceedings{Alpaka,
  author    = {Erik Zenker and Benjamin Worpitz and Ren{\'{e}} Widera
               and Axel Huebl and Guido Juckeland and
               Andreas Kn{\"{u}}pfer and Wolfgang E. Nagel and Michael Bussmann},
  title     = {Alpaka - An Abstraction Library for Parallel Kernel Acceleration},
  archivePrefix = "arXiv",
  eprint    = {1602.08477},
  keywords  = {Computer science;CUDA;Mathematical Software;nVidia;OpenMP;Package;
               performance portability;Portability;Tesla K20;Tesla K80},
  day       = {23},
  month     = {May},
  year      = {2016},
  publisher = {IEEE Computer Society},
  url       = {http://arxiv.org/abs/1602.08477},
}

@INPROCEEDINGS{Raja,
  author={Beckingsale, David A. and Burmark, Jason and Hornung, Rich and Jones, Holger and Killian, William and Kunen, Adam J. and Pearce, Olga and Robinson, Peter and Ryujin, Brian S. and Scogland, Thomas RW},
  booktitle={2019 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)}, 
  title={RAJA: Portable Performance for Large-Scale Scientific Applications}, 
  year={2019},
  volume={},
  number={},
  pages={71-81},
  keywords={C++ languages;Graphics processing units;Programming;Production;Computer architecture;Kernel;Libraries},
  doi={10.1109/P3HPC49587.2019.00012}}

@inproceedings{Sycl,
  title={SYCL: Single-source C++ accelerator programming},
  author={Ruym{\'a}n Reyes and Victor Lom{\"u}ller},
  booktitle={International Conference on Parallel Computing},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:6979118}
}

@misc{SYCL2020,
  author       = "{Khronos Group}",
  title        = "{SYCL 2020 Specification}",
  year         = {2020},
  url          = {https://www.khronos.org/registry/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf}
}

@manual{DPCPP,
  title = {Data Parallel C++ (DPC++)},
  author = {Intel Corporation},
  year = {2024},
  url = {https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html}
}

@manual{ComputeCPP,
  title = {ComputeCPP},
  author = {Codeplay Software Ltd.},
  year = {2024},
  url = {https://developer.codeplay.com/products/computecpp}
}

@inproceedings{triSYCL,
author = {Keryell, Ronan and Yu, Lin-Ya},
title = {Early experiments using SYCL single-source modern C++ on Xilinx FPGA: Extended Abstract of Technical Presentation},
year = {2018},
isbn = {9781450364393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3204919.3204937},
doi = {10.1145/3204919.3204937},
abstract = {Heterogeneous computing is required in systems ranging from low-end embedded systems up to the high-end HPC systems to reach high-performance while keeping power consumption low. Having more and more CPU and accelerators such as FPGA creates challenges for the programmer, requiring even more expertise of them. Fortunately, new modern C++-based domain-specific languages, such as the SYCL open standard from Khronos Group, simplify the programming at the full system level while keeping high performance.SYCL is a single-source programming model providing a task graph of heterogeneous kernels that can be run on various accelerators or even just the CPU. The memory heterogeneity is abstracted through buffer objects and the memory usage is abstracted with accessor objects. From these accessors, the task graph is implicitly constructed, the synchronizations and the data movements across the various physical memories are done automatically, by opposition to OpenCL or CUDA.triSYCL is an on-going open-source project used to experiment with the SYCL standard, based on C++17, OpenCL, OpenMP and Clang/LLVM. We have extended this framework to target Xilinx SDx tool to compile some SYCL programs to run on a CPU host connected to some FPGA PCIe cards, by using OpenCL and SPIR standards from Khronos.While SYCL provides functional portability, we made a few FPGA-friendly extensions to express some optimization to the SDx back-end in a pure C++ way.We present some interesting preliminary results with simple benchmarks showing how to express pipeline, dataflow and array-partitioning and we compare with the implementation written using other languages available for Xilinx FPGA: HLS C++ and OpenCL C.},
booktitle = {Proceedings of the International Workshop on OpenCL},
articleno = {18},
numpages = {8},
keywords = {triSYCL, reconfigurable computing, SYCL, SPIR, OpenCL, LLVM, FPGA, DSeL, Clang, C++17},
location = {Oxford, United Kingdom},
series = {IWOCL '18}
}

@inproceedings{neoSYCL,
author = {Ke, Yinan and Agung, Mulya and Takizawa, Hiroyuki},
title = {neoSYCL: a SYCL implementation for SX-Aurora TSUBASA},
year = {2021},
isbn = {9781450388429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3432261.3432268},
doi = {10.1145/3432261.3432268},
abstract = {Recently, the high-performance computing world has moved to more heterogeneous architectures. Thus, it has become a standard practice to offload a part of application execution to dedicated accelerators. However, the disadvantage in productivity is still a problem in programming for accelerators. This paper proposes neoSYCL: a SYCL implementation for SX-Aurora TSUBASA, aiming to improve productivity and achieve comparable performance with native implementations. Unlike other implementations, neoSYCL can identify and separate the kernel part of the SYCL code at the source code level. Thus, this approach can easily be moved to any heterogeneous architectures using the offload programming model. In this paper, we show the evaluation results on SX-Aurora TSUBASA. To quantitatively discuss not only performance but also the productivity, we use two different benchmarks and code-complexity metrics for the evaluation. The results show that neoSYCL can improve productivity while reaching the same performance as native implementations.},
booktitle = {The International Conference on High Performance Computing in Asia-Pacific Region},
pages = {50–57},
numpages = {8},
keywords = {SYCL, NEC SX-Aurora, LLVM, Heterogeneous computing},
location = {Virtual Event, Republic of Korea},
series = {HPCAsia '21}
}

@inproceedings{hipSYCL,
author = {Alpay, Aksel and Heuveline, Vincent},
year = {2020},
month = {04},
pages = {1-1},
title = {SYCL beyond OpenCL: The architecture, current state and future direction of hipSYCL},
doi = {10.1145/3388333.3388658}
}

@inproceedings{KoSYCL,
author = {Arndt, Daniel and Lebrun-Grandie, Damien and Trott, Christian},
title = {Experiences with implementing Kokkos’ SYCL backend},
year = {2024},
isbn = {9798400717901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648115.3648118},
doi = {10.1145/3648115.3648118},
abstract = {With the recent diversification of the hardware landscape in the high-performance computing community, performance-portability solutions are becoming more and more important. One of the most popular choices is Kokkos. In this paper, we describe how Kokkos maps to SYCL 2020, how SYCL had to evolve to enable a full Kokkos implementation, and where we still rely on extensions provided by Intel’s oneAPI implementation. Furthermore, we describe how applications can use Kokkos and its ecosystem to already explore upcoming C++ features also when using the SYCL backend. Finally, we are providing some performance benchmarks comparing native SYCL and Kokkos and also discuss hierarchical parallelism in the SYCL 2020 interface.},
booktitle = {Proceedings of the 12th International Workshop on OpenCL and SYCL},
articleno = {4},
numpages = {11},
keywords = {Performance portability, exascale, heterogeneous computing, high-performance computing, programming models},
location = {Chicago, IL, USA},
series = {IWOCL '24}
}

@inproceedings{RaSYCL,
author = {Homerding, Brian and Vargas, Arturo and Scogland, Tom and Chen, Robert and Davis, Mike and Hornung, Rich},
title = {Enabling RAJA on Intel GPUs with SYCL},
year = {2024},
isbn = {9798400717901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3648115.3648131},
doi = {10.1145/3648115.3648131},
abstract = {To achieve high performance, modern HPC systems take advantage of heterogeneous GPU architectures. Often these GPUs are programmed using a vendor preferred parallel programming model. Unfortunately, this often results in application code that is not portable across vendors. To address this issue, open parallel programming models have been introduced. One such parallel programming model is provided by the RAJA Portability Suite. RAJA is a portability layer that provides an abstract application developer API as a library through modern C++. In RAJA, computational kernels are lowered down to a backend language appropriate for the target architecture. Additionally, RAJA is designed to provide control to the application developer over kernel execution while minimizing modification to the application source code. In this work, we describe the SYCL backend implementation in RAJA for Intel GPU execution. We discuss the implementation of various features in the SYCL backend along with the challenges and lessons learned. Finally, we investigate the performance impact of executing several HPC kernels through RAJA when compared to direct SYCL implementations.},
booktitle = {Proceedings of the 12th International Workshop on OpenCL and SYCL},
articleno = {7},
numpages = {10},
keywords = {Parallel Programming, RAJA, SYCL},
location = {Chicago, IL, USA},
series = {IWOCL '24}
}

@inproceedings{Breyer,
author = {Breyer, Marcel and Van Craen, Alexander and Pfl\"{u}ger, Dirk},
title = {A Comparison of SYCL, OpenCL, CUDA, and OpenMP for Massively Parallel Support Vector Machine Classification on Multi-Vendor Hardware},
year = {2022},
isbn = {9781450396585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3529538.3529980},
doi = {10.1145/3529538.3529980},
abstract = {In scientific computing and Artificial Intelligence (AI), which both rely on massively parallel tasks, frameworks like the Compute Unified Device Architecture (CUDA) and the Open Computing Language (OpenCL) are widely used to harvest the computational power of accelerator cards, in particular of Graphics Processing Units (GPUs). A few years ago, GPUs from NVIDIA were used almost exclusively for these tasks but meanwhile, AMD and Intel are increasing their shares of the GPUs market. This introduces many new challenges for code development, as the prevailing CUDA code can only run on NVIDIA hardware and must be adapted or even completely rewritten to run on GPUs from AMD or Intel. In this paper, we compare the different competing programming frameworks OpenMP, CUDA, OpenCL, and SYCL, paying special attention to the two SYCL implementations hipSYCL and DPC++. Thereby, we investigate the different frameworks with respect to their usability, performance, and performance portability on a variety of hardware platforms from different vendors, i.e., GPUs from NVIDIA, AMD, and Intel and Central Processing Units (CPUs) from AMD and Intel. Besides discussing the runtimes of these frameworks on the different hardware platforms, we also focus our comparison on the differences between the nd_range kernel formulation and the SYCL specific hierarchical kernels. Our Parallel Least Squares Support Vector Machine (PLSSVM) library implements backends for the four previously mentioned programming frameworks for a Least Squares Support Vector Machines (LS-SVMs). At its example, we show which of the frameworks is best suited for a standard workload that is frequently employed in scientific computing and AI, depending on the target hardware: The most computationally intensive part of our PLSSVM library is solving a system of linear equations using the Conjugate Gradient (CG) method. Specifically, we parallelize the implicit matrix-vector multiplication inside the CG method, a workload common in many scientific codes. The PLSSVM code, utility scripts, and documentation are all available on GitHub: https://github.com/SC-SGS/PLSSVM.},
booktitle = {Proceedings of the 10th International Workshop on OpenCL},
articleno = {2},
numpages = {12},
keywords = {CPU, CUDA, GPU, Machine Learning, OpenCL, OpenMP, Performance Evaluation, Performance Portability, SVM, SYCL},
location = {Bristol, United Kingdom, United Kingdom},
series = {IWOCL '22}
}

@article{ZHANG2014126,
title = {Parallel computation of a dam-break flow model using OpenMP on a multi-core computer},
journal = {Journal of Hydrology},
volume = {512},
pages = {126-133},
year = {2014},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2014.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0022169414001437},
author = {Shanghong Zhang and Zhongxi Xia and Rui Yuan and Xiaoming Jiang},
keywords = {Dam-break flow, Parallel computing, OpenMP, Finite volume method},
abstract = {Summary
High-performance calculations are of great importance to the simulation of dam-break events, as discontinuous solutions and accelerated speed are key factors in the process of dam-break flow modeling. In this study, Roe’s approximate Riemann solution of the finite volume method is adopted to solve the interface flux of grid cells and accurately simulate the discontinuous flow, and shared memory technology (OpenMP) is used to realize parallel computing. Because an explicit discrete technique is used to solve the governing equations, and there is no correlation between grid calculations in a single time step, the parallel dam-break model can be easily realized by adding OpenMP instructions to the loop structure of the grid calculations. The performance of the model is analyzed using six computing cores and four different grid division schemes for the Pangtoupao flood storage area in China. The results show that the parallel computing improves precision and increases the simulation speed of the dam-break flow, the simulation of 320h flood process can be completed within 1.6h on a 16-kernel computer; a speedup factor of 8.64× is achieved. Further analysis reveals that the models involving a larger number of calculations exhibit greater efficiency and a higher rate of acceleration. At the same time, the model has good extendibility, as the speedup increases with the number of processor cores. The parallel model based on OpenMP can make full use of multi-core processors, making it possible to simulate dam-break flows in large-scale watersheds on a single computer.}
}

@manual{GCC,
  title = {GNU Compiler Collection (GCC)},
  author = {Free Software Foundation, Inc.},
  year = {2024},
  url = {https://gcc.gnu.org/}
}

@inproceedings{sycl_bench,
author = {Crisci, Luigi and Carpentieri, Lorenzo and Thoman, Peter and Alpay, Aksel and Heuveline, Vincent and Cosenza, Biagio},
title = {SYCL-Bench 2020: Benchmarking SYCL 2020 on AMD, Intel, and NVIDIA GPUs},
year = {2024},
isbn = {9798400717901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3648115.3648120},
doi = {10.1145/3648115.3648120},
abstract = {Today, the SYCL standard represents the most advanced programming model for heterogeneous computing, delivering both productivity, portability, and performance in pure C++17. SYCL 2020, in particular, represents a major enhancement that pushes the boundaries of heterogeneous programming by introducing a number of new features. As the new features are implemented by existing compilers, it becomes critical to assess the maturity of the implementation through accurate and specific benchmarking. This paper presents SYCL-Bench 2020, an extended benchmark suite specifically designed to evaluate six key features of SYCL 2020: unified shared memory, reduction kernel, specialization constants, group algorithms, in-order queue, and atomics. We experimentally evaluate SYCL-Bench 2020 on GPUs from the three major vendors, i.e., AMD, Intel, and NVIDIA, and on two different SYCL implementations AdaptiveCPP and oneAPI DPC++.},
booktitle = {Proceedings of the 12th International Workshop on OpenCL and SYCL},
articleno = {1},
numpages = {12},
keywords = {GPU, HPC, SYCL, benchmark, heterogeneous computing, portability},
location = {Chicago, IL, USA},
series = {IWOCL '24}
}

@inproceedings{sycl_swe,
author = {B\"{u}ttner, Markus and Alt, Christoph and Kenter, Tobias and K\"{o}stler, Harald and Plessl, Christian and Aizinger, Vadym},
title = {Enabling Performance Portability for Shallow Water Equations on CPUs, GPUs, and FPGAs with SYCL},
year = {2024},
isbn = {9798400706394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3659914.3659925},
doi = {10.1145/3659914.3659925},
abstract = {In order to make the best use of the diverse hardware architectures in present and future high-performance computers, developers and maintainers of scientific simulation codes strive for performance portability. The goal is to reach a good fraction of the hardware-specific practically achievable performance while maintaining a largely unified codebase. In benchmarks and first production codes, SYCL has been demonstrated to be a promising programming model for this purpose when targeting different CPU and GPUs.In this work, we utilize SYCL to develop a performance portable implementation of the 2D shallow water equations, discretized on unstructured triangular meshes using the discontinuous Galerkin method with polynomial orders zero, one, and two. In addition to GPUs from three and CPUs from two vendors, we also broaden the scope of target architectures by including Intel Stratix FPGAs with a fundamentally different execution model. We show that with a few targeted and encapsulated specializations, it is possible to adapt the execution flow to the respective targets. The performance analysis shows how FPGAs complement the other two architectures with particularly good performance for small problem sizes.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {11},
numpages = {12},
keywords = {SYCL, performance-portability, shallow water equations, discontinous Galerkin method},
location = {Zurich, Switzerland},
series = {PASC '24}
}

@inproceedings{sycl_bench2,
author = {Breyer, Marcel and Van Craen, Alexander and Pfl\"{u}ger, Dirk},
title = {A Comparison of SYCL, OpenCL, CUDA, and OpenMP for Massively Parallel Support Vector Machine Classification on Multi-Vendor Hardware},
year = {2022},
isbn = {9781450396585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3529538.3529980},
doi = {10.1145/3529538.3529980},
abstract = {In scientific computing and Artificial Intelligence (AI), which both rely on massively parallel tasks, frameworks like the Compute Unified Device Architecture (CUDA) and the Open Computing Language (OpenCL) are widely used to harvest the computational power of accelerator cards, in particular of Graphics Processing Units (GPUs). A few years ago, GPUs from NVIDIA were used almost exclusively for these tasks but meanwhile, AMD and Intel are increasing their shares of the GPUs market. This introduces many new challenges for code development, as the prevailing CUDA code can only run on NVIDIA hardware and must be adapted or even completely rewritten to run on GPUs from AMD or Intel. In this paper, we compare the different competing programming frameworks OpenMP, CUDA, OpenCL, and SYCL, paying special attention to the two SYCL implementations hipSYCL and DPC++. Thereby, we investigate the different frameworks with respect to their usability, performance, and performance portability on a variety of hardware platforms from different vendors, i.e., GPUs from NVIDIA, AMD, and Intel and Central Processing Units (CPUs) from AMD and Intel. Besides discussing the runtimes of these frameworks on the different hardware platforms, we also focus our comparison on the differences between the nd_range kernel formulation and the SYCL specific hierarchical kernels. Our Parallel Least Squares Support Vector Machine (PLSSVM) library implements backends for the four previously mentioned programming frameworks for a Least Squares Support Vector Machines (LS-SVMs). At its example, we show which of the frameworks is best suited for a standard workload that is frequently employed in scientific computing and AI, depending on the target hardware: The most computationally intensive part of our PLSSVM library is solving a system of linear equations using the Conjugate Gradient (CG) method. Specifically, we parallelize the implicit matrix-vector multiplication inside the CG method, a workload common in many scientific codes. The PLSSVM code, utility scripts, and documentation are all available on GitHub: https://github.com/SC-SGS/PLSSVM.},
booktitle = {Proceedings of the 10th International Workshop on OpenCL},
articleno = {2},
numpages = {12},
keywords = {CPU, CUDA, GPU, Machine Learning, OpenCL, OpenMP, Performance Evaluation, Performance Portability, SVM, SYCL},
location = {Bristol, United Kingdom, United Kingdom},
series = {IWOCL '22}
}

@inproceedings{SSCP,
author = {Alpay, Aksel and Heuveline, Vincent},
title = {One Pass to Bind Them: The First Single-Pass SYCL Compiler with Unified Code Representation Across Backends},
year = {2023},
isbn = {9798400707452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3585341.3585351},
doi = {10.1145/3585341.3585351},
abstract = {Current SYCL implementations rely on multiple compiler invocations to generate code for host and device, and typically even employ one compiler invocation per required backend code format such as SPIR-V, PTX or amdgcn. This makes generating “universal” binaries that can run on all devices supported by a SYCL implementation very time-consuming, or outright impractical. The ability to generate such universal binaries is however important e.g. when a software vendor wishes to distribute binaries to users that rely on unknown hardware configurations. To address this issue, we present the very first SYCL implementation with a single-source, single compiler pass (SSCP) design and a unified code representation across backends. This allows a single compiler invocation to generate a binary that can execute kernels on all supported devices, dramatically reducing both compile times as well as the user effort required to generate such universal binaries. Our work is publicly available as part of the hipSYCL implementation of SYCL, and supports Intel GPUs through SPIR-V, NVIDIA GPUs through CUDA PTX and AMD GPUs through ROCm amdgcn code. Our new compiler operates in two phases: At compile time, during the regular host compilation pass, it extracts the LLVM IR of kernels. This IR is then stored in a backend-independent fashion in the host binary. At runtime, the embedded LLVM IR is then lowered to the format required by backend drivers (e.g. PTX, SPIR-V, amdgcn). This approach enables portability of a single code representation even if backends do not support a common code format, while still allowing interoperability with vendor-specific optimized libraries. We find that our new compiler can generate highly portable binaries that run on any NVIDIA, Intel or AMD ROCm GPU with only 20\% additional compilation time compared to a regular clang host compilation. On our test system, this is roughly 2.2 \texttimes{} faster than compiling with the existing hipSYCL compiler for just three AMD GPUs. We also show that the cost of the additional runtime compilation steps can be expected to be approximately comparable to the cost of runtime compilation that backend drivers already perform today, e.g. to lower SPIR-V to machine code. Lastly, we present early performance results on four different GPUs from three vendors. We find that performance is usually within 10\% of current multipass SYCL compiler techniques, with the maximum deviations ranging from a performance regression of 13\% to a speedup of 27\%. This implies that compared to current SYCL compilation techniques, our new compiler achieves similar performance while substantially decreasing compile times, and increasing the portability of generated binaries.},
booktitle = {Proceedings of the 2023 International Workshop on OpenCL},
articleno = {7},
numpages = {12},
keywords = {C++, CUDA, GPU, HIP, LLVM, SPIR-V, SYCL, compilers, heterogeneity, oneAPI, parallelism, parallelruntimes},
location = {Cambridge, United Kingdom},
series = {IWOCL '23}
}

@article{NEAL2010,
title = {A comparison of three parallelisation methods for 2D flood inundation models},
journal = {Environmental Modelling & Software},
volume = {25},
number = {4},
pages = {398-411},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209002965},
author = {Jeffrey C. Neal and Timothy J. Fewtrell and Paul D. Bates and Nigel G. Wright},
keywords = {Hydraulic model, LISFLOOD-FP, Urban flooding, OpenMP, MPI, SIMD, ClearSpeed},
abstract = {For many applications two-dimensional hydraulic models are time intensive to run due to their computational requirements, which can adversely affect the progress of both research and industry modelling projects. Computational time can be reduced by running a model in parallel over multiple cores. However, there are many parallelisation methods and these differ in terms of difficulty of implementation, suitability for particular codes and parallel efficiency. This study compares three parallelisation methods based on OpenMP, message passing and specialised accelerator cards. The parallel implementations of the codes were required to produce near identical results to a serial version for two urban inundation test cases. OpenMP was considered the easiest method to develop and produced similar speedups (of ∼3.9×) to the message passing code on up to four cores for a fully wet domain. The message passing code was more efficient than OpenMP, and remained over 90% efficient on up to 50 cores for a completely wet domain. All parallel codes were less efficient for a partially wet domain test case. The accelerator card code was faster and more power efficient than the standard code on a single core for a fully wet domain, but was subject to longer development time (2 months compared to <2 week for the other methods).}
}

@article{DELIS2009,
title = {A finite volume method parallelization for the simulation of free surface shallow water flows},
journal = {Mathematics and Computers in Simulation},
volume = {79},
number = {11},
pages = {3339-3359},
year = {2009},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2009.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378475409001633},
author = {A.I. Delis and E.N. Mathioudakis},
keywords = {Shallow water equations, Finite volume, Domain decomposition, Distributed memory parallel architectures, Grid computing},
abstract = {We construct a parallel algorithm, suitable for distributed memory architectures, of an explicit shock-capturing finite volume method for solving the two-dimensional shallow water equations. The finite volume method is based on the very popular approximate Riemann solver of Roe and is extended to second order spatial accuracy by an appropriate TVD technique. The parallel code is applied to distributed memory architectures using domain decomposition techniques and we investigate its performance on a grid computer and on a Distributed Shared Memory supercomputer. The effectiveness of the parallel algorithm is considered for specific benchmark test cases. The performance of the realization measured in terms of execution time and speedup factors reveals the efficiency of the implementation.}
}

@article{RAO2004,
title = {A parallel hydrodynamic model for shallow water equations},
journal = {Applied Mathematics and Computation},
volume = {150},
number = {1},
pages = {291-302},
year = {2004},
issn = {0096-3003},
doi = {https://doi.org/10.1016/S0096-3003(03)00228-5},
url = {https://www.sciencedirect.com/science/article/pii/S0096300303002285},
author = {Prasada Rao},
keywords = {Open channel flows, MPI, Domain decomposition, Speedup, Efficiency},
abstract = {A parallel implementation of a finite difference model for solving two-dimensional, time-dependent, open channel flows is presented. The algebraic equations resulting from the finite difference discretization of the two dimensional shallow water flow equations are solved by using explicit MacCormack scheme. The parallel code has been implemented on distributed–shared memory system, by using domain decomposition techniques. The message passing interface (MPI) protocols are incorporated for inter processor data communication. The effect of using two different geometry partitions is investigated. A comparison of the wallclock time of the code between these two partitions is made, and code performances with respect to different number of processors are presented.}
}

@article{CASTRO2006,
title = {A parallel 2d finite volume scheme for solving systems of balance laws with nonconservative products: Application to shallow flows},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {195},
number = {19},
pages = {2788-2815},
year = {2006},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2005.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782505003324},
author = {M.J. Castro and J.A. García-Rodríguez and J.M. González-Vida and C. Parés},
keywords = {Parallelization, Domain decomposition, Finite volume schemes, Conservation laws, Source terms, Nonconservative products, Shallow water systems, Two-layer problems, Geophysical flows, Strait of Gibraltar},
abstract = {The goal of this paper is to construct parallel solvers for 2d hyperbolic systems of conservation laws with source terms and nonconservative products. More precisely, finite volumes solvers on nonstructured grids are considered. The method of lines is applied: at every intercell a projected Riemann problem along the normal direction is considered which is discretized by means of the numerical schemes presented in [M.J. Castro, J. Macías, C. Parés. A Q-scheme for a class of systems of coupled conservation laws with source term. Application to a two-layer 1-D shallow water system, ESAIM: M2AN 35 (1) (2001) 107–127]. The resulting 2d numerical schemes are explicit and first order accurate. The solver is next parallelized by a domain decomposition technique. The specific application of the scheme to one- and two-layer shallow water systems has been implemented on a PC’s cluster. An efficient data structure based on OOMPI (C++ object oriented extension of MPI) has been developed to optimize the data exchange among the processors. Some numerical tests are next presented to validate the solver and the performance of its parallel implementation. Finally the two-layer shallow water model is applied to the simulation of the steady exchange through the Strait of Gibraltar.}
}

@article{HERVOUET2000,
author = {Hervouet, J-M.},
title = {A high resolution 2-D dam-break model using parallelization},
journal = {Hydrological Processes},
volume = {14},
number = {13},
pages = {2211-2230},
keywords = {dam-break simulation, finite elements, parallel processing},
doi = {https://doi.org/10.1002/1099-1085(200009)14:13<2211::AID-HYP24>3.0.CO;2-8},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-1085%28200009%2914%3A13%3C2211%3A%3AAID-HYP24%3E3.0.CO%3B2-8},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1099-1085%28200009%2914%3A13%3C2211%3A%3AAID-HYP24%3E3.0.CO%3B2-8},
abstract = {Abstract The evolution of laws on dam safety in France is briefly described. Most simulation studies for such problems are currently attempted using one-dimensional models, and we investigate here the possibility of moving to two-dimensional simulations. To date, these have been constrained by lack of available computational power. Parallel processing is an obvious solution, however, to date, an efficient parallel processing method that does not require major software recoding has proved elusive. Domain-decomposition is identified as being capable of overcoming these problems, as it allows pre-existing software to be adapted to run on clusters of supercomputer processors, workstations or PCs. The development of this new parallel method is outlined and its accuracy and efficiency tested in terms of the TELEMAC-2D model applied to the Malpasset dam break accident, which occurred in 1959 in the south of France. After a discussion of the data available, a sensitivity study is performed to evaluate some physical parameters in the equations, mainly the diffusion coefficient and the bottom friction. The friction appears to be the most important. A discrepancy of 3·4\% on the wave celerity is obtained between the model and field observations for a number of locations where the arrival time of the flood wave is perfectly known. The efficiency of different scalar and parallel machines is assessed. The study concludes that 2-D simulations of flood waves are already possible on domains with a length of some 10 s of kilometres. Larger domains (100 to 400 km) are also within reach with supercomputers, or with parallel architectures. Copyright © 2000 John Wiley \& Sons, Ltd.},
year = {2000}
}

@article{PAU2006,
author = {Pau, John and Sanders, Brett},
year = {2006},
month = {03},
pages = {},
title = {Performance of Parallel Implementations of an Explicit Finite-Volume Shallow-Water Model},
volume = {20},
journal = {Journal of Computing in Civil Engineering - J COMPUT CIVIL ENG},
doi = {10.1061/(ASCE)0887-3801(2006)20:2(99)}
}

@article{NEAL2009,
title = {Parallelisation of storage cell flood models using OpenMP},
journal = {Environmental Modelling & Software},
volume = {24},
number = {7},
pages = {872-877},
year = {2009},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2008.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815208002363},
author = {Jeffrey Neal and Timothy Fewtrell and Mark Trigg},
keywords = {Hydraulic modelling, Storage cells, LISFLOOD-FP, OpenMP, Parallelisation, Flooding, HPC},
abstract = {This paper describes the implementation and benchmarking of a parallel version of the LISFLOOD-FP hydraulic model based on the OpenMP Application Programming Interface. The motivation behind the study was that reducing model run time through parallelisation would increase the utility of such models by expanding the domains over which they can be practically implemented, allowing previously inaccessible scientific questions to be addressed. Parallel speedup was calculated for 13 models distributed over seven study sites and implemented on one, two, four and in selected cases eight processor cores. The models represent a range of previous applications from large area, coarse resolution models of the Amazon, to fine resolution models of urban areas, to orders of magnitude smaller models of rural floodplains. Parallel speedups were greater for larger model domains, especially for models with over 0.2–0.4 million cells where parallel efficiencies of up to 0.75 on four and eight cores were achieved. A key advantage of using OpenMP and an explicit rather than implicit model was the ease of implementation and minimal code changes required to run simulations in parallel.}
}

@inproceedings{LAMB2009,
  title={A fast 2D floodplain inundation model},
  author={Rob Lamb and Amanda Crossley and S. Travis Waller},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:61732991}
}

@inproceedings{WRIGHT2006,
author = {Villanueva, Ignacio and Wright, Nigel},
year = {2006},
month = {01},
pages = {},
title = {An efficient multi-processor solver for the 2D shallow water equations}
}

@manual{MicrosoftDX9,
  author    = {Microsoft},
  title     = {Microsoft DirectX 9.0 Software Development Kit},
  year      = {2002},
  note      = {Microsoft Corporation}
}

@manual{ClearSpeed2007,
  author    = {ClearSpeed},
  year      = {2007},
  title     = {CSX Processor Architecture},
  organization = {ClearSpeed Technology},
  address   = {Bristol, UK},
  url       = {http://developer.clearspeed.com/resources/library/}
}

@article{Sanders,
author = {Sanders, Brett and Schubert, Jochen and Detwiler, Russ},
year = {2010},
month = {12},
pages = {},
title = {ParBreZo: A parallel, unstructured grid, Godunov-type, shallow-water code for high-resolution flood inundation modeling at the regional scale},
volume = {33},
journal = {Advances in Water Resources - ADV WATER RESOUR},
doi = {10.1016/j.advwatres.2010.07.007}
}

@article{LASTRA2009,
title = {Simulation of shallow-water systems using graphics processing units},
journal = {Mathematics and Computers in Simulation},
volume = {80},
number = {3},
pages = {598-618},
year = {2009},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2009.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0378475409003139},
author = {Miguel Lastra and José M. Mantas and Carlos Ureña and Manuel J. Castro and José A. García-Rodríguez},
keywords = {Shallow-water simulation, General-purpose computation on graphics processing units (GPGPU), High performance scientific computing},
abstract = {This paper addresses the speedup of the numerical solution of shallow-water systems in 2D domains by using modern graphics processing units (GPUs). A first order well-balanced finite volume numerical scheme for 2D shallow-water systems is considered. The potential data parallelism of this method is identified and the scheme is efficiently implemented on GPUs for one-layer shallow-water systems. Numerical experiments performed on several GPUs show the high efficiency of the GPU solver in comparison with a highly optimized implementation of a CPU solver.}
}

@article{Brodtkorb2013,
author = {Brodtkorb, André and Hagen, Trond and Sætra, Martin},
year = {2013},
month = {01},
pages = {4–13},
title = {Graphics processing unit (GPU) programming strategies and trends in GPU computing},
volume = {73},
journal = {Journal of Parallel and Distributed Computing},
doi = {10.1016/j.jpdc.2012.04.003}
}

@article{HAGEN2005,
author = {Hagen, Trond and Hjelmervik, Jon and Lie, Knut-Andreas and Natvig, Jostein and Henriksen, M.},
year = {2005},
month = {11},
pages = {716-726},
title = {Visual simulation of shallow-water waves},
volume = {13},
journal = {Simulation Modelling Practice and Theory},
doi = {10.1016/j.simpat.2005.08.006}
}

@book{OpenGL,
  author    = {Dave Shreiner and Graham Sellers and John Kessenich and Bill Licea-Kane},
  title     = {OpenGL Programming Guide: The Official Guide to Learning OpenGL, Versions 4.3 and Later},
  edition   = {8},
  publisher = {Addison-Wesley},
  year      = {2013}
}

@book{Cg,
  author    = {Randima Fernando and Mark J. Kilgard},
  title     = {The Cg Tutorial: The Definitive Guide to Programmable Real-Time Graphics},
  publisher = {Addison-Wesley},
  year      = {2003}
}

@article{CASTRO2011,
title = {GPU computing for shallow water flow simulation based on finite volume schemes},
journal = {Comptes Rendus Mécanique},
volume = {339},
number = {2},
pages = {165-184},
year = {2011},
note = {High Performance Computing},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1631072110002147},
author = {Manuel J. Castro and Sergio Ortega and Marc {de la Asunción} and José M. Mantas and José M. Gallardo},
keywords = {Computer science, GPUs, Finite volume methods, Shallow water, High-order schemes},
abstract = {This article is a review of the work that we are carrying out to efficiently simulate shallow water flows. In this paper, we focus on the efficient implementation of path-conservative Roe type high-order finite volume schemes to simulate shallow flows that are supposed to be governed by the one-layer or two-layer shallow water systems, formulated under the form of a conservation law with source terms. The implementation of the scheme is carried out on Graphics Processing Units (GPUs), thus achieving a substantial improvement of the speedup with respect to normal CPUs. Finally, some numerical experiments are presented.}
}

@inproceedings{LIANG2009,
author = {Liang, Wen-Yew and Hsieh, Tung-Ju and Satria, Muhammad T and Chang, Yang-Lang and Fang, Jyh-Perng and Chen, Chih-Chia and Han, Chin-Chuan},
year = {2009},
month = {06},
pages = {593-603},
title = {A GPU-Based Simulation of Tsunami Propagation and Inundation},
isbn = {978-3-642-03094-9},
doi = {10.1007/978-3-642-03095-6_56}
}

@article{Brodtkorb2010,
author = {Brodtkorb, André and Hagen, Trond and Lie, Knut-Andreas and Natvig, Jostein},
year = {2010},
month = {10},
pages = {341-353},
title = {Simulation and visualization of the Saint-Venant system using GPUs},
volume = {13},
journal = {Computing and Visualization in Science},
doi = {10.1007/s00791-010-0149-x}
}

@article{dlA2011,
author = {de la Asunción, Marc and Mantas, José and Castro, Manuel},
year = {2011},
month = {11},
pages = {206-214},
title = {Simulation of one-layer shallow water systems on multicore and CUDA architectures},
volume = {58},
journal = {The Journal of Supercomputing},
doi = {10.1007/s11227-010-0406-2}
}

@article{KALYANAPU2011,
title = {Assessment of GPU computational enhancement to a 2D flood model},
journal = {Environmental Modelling & Software},
volume = {26},
number = {8},
pages = {1009-1016},
year = {2011},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2011.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364815211000582},
author = {Alfred J. Kalyanapu and Siddharth Shankar and Eric R. Pardyjak and David R. Judi and Steven J. Burian},
keywords = {2D flood model, Flood simulation, GPU programming, CUDA},
abstract = {This paper presents a study of the computational enhancement of a Graphics Processing Unit (GPU) enabled 2D flood model. The objectives are to demonstrate the significant speedup of a new GPU-enabled full dynamic wave flood model and to present the effect of model spatial resolution on its speedup. A 2D dynamic flood model based on the shallow water equations is parallelized using the GPU approach developed in NVIDIA’s Compute Unified Development Architecture (CUDA). The model is validated using observations of the Taum Sauk pump storage hydroelectric power plant dam break flood event. For the Taum Sauk flood simulation, the GPU model speedup compared to an identical CPU model implementation is 80×–88× for computational domains ranging from 65.5 k to 1.05 M cells. Thirty minutes of event time were simulated by the GPU model in 2 min, 15 times faster than real time. An important finding of the analysis of model domain size is the GPU model is not constrained by model domain extent as is the CPU model. Finally, the GPU implementation is shown to be scalable compared with the CPU version, an important characteristic for large domain flood modeling studies.}
}

@article{DELAASUNCION2013441,
title = {Efficient GPU implementation of a two waves TVD-WAF method for the two-dimensional one layer shallow water system on structured meshes},
journal = {Computers & Fluids},
volume = {80},
pages = {441-452},
year = {2013},
note = {Selected contributions of the 23rd International Conference on Parallel Fluid Dynamics ParCFD2011},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2012.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0045793012000217},
author = {Marc {de la Asunción} and Manuel J. Castro and E.D. Fernández-Nieto and José M. Mantas and Sergio Ortega Acosta and José Manuel González-Vida},
keywords = {Finite volume schemes, TVD-WAF scheme, CUDA, Shallow-water equations},
abstract = {The numerical solutions of shallow water equations are useful for applications related to geophysical flows that usually take place in large computational domains and could require real time calculation. Therefore, parallel versions of accurate and efficient numerical solvers for high performance platforms are needed to be able to deal with these simulation scenarios in reasonable times. In this paper we present an efficient CUDA implementation of a first and second order HLL methods and a two-waves TVD-WAF one. We propose to write all these methods under a common framework, such as, their CUDA implementations share the same structure. In particular, the reformulation of TVD-WAF numerical flux and the improved definition of the flux limiter allows us to obtain a more robust solver in situations like wet/dry fronts. Finally, some numerical tests are presented showing that the TVD-WAF method is slightly slower that the first order HLL method and two times faster than the second order HLL method, but it provides numerical results almost as accurate as the second order HLL scheme.}
}

@article{VACONDIO2017119,
title = {A non-uniform efficient grid type for GPU-parallel Shallow Water Equations models},
journal = {Environmental Modelling & Software},
volume = {88},
pages = {119-137},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216309252},
author = {Renato Vacondio and Alessandro {Dal Palù} and Alessia Ferrari and Paolo Mignosa and Francesca Aureli and Susanna Dazzi},
abstract = {A GPU-parallel numerical model for the solution of the 2D Shallow Water Equations, based on a novel type of grid called Block-Uniform Quadtree (BUQ), is presented. BUQ grids are based on a data structure which allows to exploit the computational capability of GPUs with minimum overheads, while discretizing the domain with non-uniform resolution. Different cases have been simulated in order to assess the efficiency of the BUQ grids. Theoretical and laboratory tests demonstrate that speed-ups of up to one order of magnitude can be achieved in comparison with uniform Cartesian grids. In the simulation of a hypothetical flood event induced by a levee breach in a real 83 km long river reach, with maximum resolution of 5 m, a ratio of physical to computational time of about 12 was obtained, opening scenarios of quasi real-time 2D simulations in large domains, still retaining a high resolution where necessary.}
}

@article{VACONDIO201460,
title = {GPU-enhanced Finite Volume Shallow Water solver for fast flood simulations},
journal = {Environmental Modelling & Software},
volume = {57},
pages = {60-75},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S136481521400053X},
author = {R. Vacondio and A. {Dal Palù} and P. Mignosa},
keywords = {Flood simulation, Parallel computing, GPU, Shallow Water, Finite Volume},
abstract = {In this paper a parallelization of a Shallow Water numerical scheme suitable for Graphics Processor Unit (GPU) architectures under the NVIDIA™'s Compute Unified Device Architecture (CUDA) framework is presented. In order to provide robust and accurate simulations of real flood events, the system features a state-of-the-art Finite Volume explicit discretization technique which is well balanced, second order accurate and based on positive depth reconstruction. The model is based on a Cartesian grid and boundary conditions are implemented by means of the implicit local ghost cell approach, which enables the discretization of a broad spectrum of boundary conditions including inflow/outflow conditions. A novel and efficient Block Deactivation Optimization procedure has also been adopted, in order to increase the efficiency of the numerical scheme in the presence of wetting-drying fronts. This led to speedups of two orders of magnitude with respect to a single-core CPU. The code has been validated against several severe benchmark test cases, and its capability of producing accurate fast simulations (with high ratios between physical and computing times) for different real world cases has been shown.}
}

@article{LACASTA20141,
title = {An optimized GPU implementation of a 2D free surface simulation model on unstructured meshes},
journal = {Advances in Engineering Software},
volume = {78},
pages = {1-15},
year = {2014},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2014.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0965997814001331},
author = {A. Lacasta and M. Morales-Hernández and J. Murillo and P. García-Navarro},
keywords = {GPU, Finite volume methods, Unsteady flow, Unstructured meshes, Dry/wet boundaries, CUDA, High performance computing},
abstract = {This work is related with the implementation of a finite volume method to solve the 2D Shallow Water Equations on Graphic Processing Units (GPU). The strategy is fully oriented to work efficiently with unstructured meshes which are widely used in many fields of Engineering. Due to the design of the GPU cards, structured meshes are better suited to work with than unstructured meshes. In order to overcome this situation, some strategies are proposed and analyzed in terms of computational gain, by means of introducing certain ordering on the unstructured meshes. The necessity of performing the simulations using unstructured instead of structured meshes is also justified by means of some test cases with analytical solution.}
}

@article{LACASTA2013225,
title = {Preprocess static subdomain decomposition in practical cases of 2D unsteady hydraulic simulation},
journal = {Computers & Fluids},
volume = {80},
pages = {225-232},
year = {2013},
note = {Selected contributions of the 23rd International Conference on Parallel Fluid Dynamics ParCFD2011},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2012.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045793012000977},
author = {A. Lacasta and P. García-Navarro and J. Burguete and J. Murillo},
keywords = {Parallelization, Computational hydraulics, Unsteady flow, Static domain decomposition, Dry/wet boundaries},
abstract = {Explicit finite volume methods are frequently used and widely accepted in hydraulic models based on the shallow water approximation. The main drawback of the approach is the time step size limit imposed by the Courant–Friedrichs–Lewy numerical stability constraint. This leads to excessively long computational times in large scale cases of practical interest. At the same time, the accuracy of the numerical results is associated to the use of fine computational meshes able to achieve enough spatial resolution. Taking into account that hydraulic modelers do not have access, in general, to large computational facilities, suitable and useful parallelization techniques are required. Furthermore, if high performance computing facilities are used, it is usually necessary to provide an estimation of the requirements of computational load to cover the length of the simulation. In this work the suitability of a preprocess static subdomain decomposition is explored and presented as a promising strategy to improve the efficiency of 2D unsteady shallow water computational models over dry bed in medium scale computational facilities and, at the same time, is useful to provide a preprocess computational time estimation if large scale computational facilities are going to be used.}
}

@article{Petaccia2016,
author={Petaccia, G.
and Leporati, F.
and Torti, E.},
title={OpenMP and CUDA simulations of Sella Zerbino Dam break on unstructured grids},
journal={Computational Geosciences},
year={2016},
month={Oct},
day={01},
volume={20},
number={5},
pages={1123-1132},
abstract={This paper presents two 2D dam break parallelized models based on shallow water equations (SWE) written in conservative form. The models were implemented exploiting multicore PC systems and graphics processor unit (GPU) architectures under the OpenMP and the NVIDIA{\texttrademark}'s compute unified device architecture (CUDA) frameworks. The mathematical model is solved using a finite-volume technique on an unstructured grid, with Roe's approximate Riemann solver, a first-order upwind scheme. The upwind treatment of the source terms is implemented. A technique to cope with a wetting-drying advance front is adopted, together with the inclusion of the influence of source terms in the stability constraint in order to prevent negative water depths at the dry fronts. The proposed model is first applied to a laboratory test and then to a real dam break that occurred in Italy in 1935. Results on different grid sizes are compared to show the computing efficiency between the original sequential model and the parallelized models.},
issn={1573-1499},
doi={10.1007/s10596-016-9580-5},
url={https://doi.org/10.1007/s10596-016-9580-5}
}

@article{CARLOTTO2021105205,
title = {SW2D-GPU: A two-dimensional shallow water model accelerated by GPGPU},
journal = {Environmental Modelling & Software},
volume = {145},
pages = {105205},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105205},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002474},
author = {Tomas Carlotto and Pedro Luiz {Borges Chaffe} and Camyla {Innocente dos Santos} and Seungsoo Lee},
keywords = {Shallow water equation, High performance computing, Lake ecosystem, Parallel computing, Flood},
abstract = {Shallow water models are used for simulating flood and lake hydrodynamics. However, the computational cost of those models is often high and require high performance computing. We present the SW2D-GPU: a two-dimensional shallow water model accelerated by General Purpose Graphics Processing Unit. The model is implemented in parallel using CUDA C/C++. We exemplify the use of the model with two case studies: (i) Flood simulation in an urban area and (ii) water level simulation in a lake catchment. We have included potential evaporation in the formulation which expands its application to water level simulations in lakes and reservoirs. The SW2D-GPU model is approximately 34 times faster than its equivalent sequential version. The model can be run in any computer equipped with a NVIDIA GPU. Integrated simulations of surface waters in lake watershed and simulations of floods caused by dam break are some of the potential applications.}
}

@article{SMITH2013334,
title = {Towards a generalised GPU/CPU shallow-flow modelling tool},
journal = {Computers & Fluids},
volume = {88},
pages = {334-343},
year = {2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2013.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0045793013003630},
author = {Luke S. Smith and Qiuhua Liang},
keywords = {Shallow water equations, Graphics processing units, Godunov-type scheme, Floods modeling, High-performance computing},
abstract = {This paper presents new software that takes advantage of modern graphics processing units (GPUs) to significantly expedite two-dimensional shallow-flow simulations when compared to a traditional central processing unit (CPU) approach. A second-order accurate Godunov-type MUSCL-Hancock scheme is used with an HLLC Riemann solver to create a robust framework suitable for different types of flood simulation. A real-world dam collapse event is simulated using a 1.8million cell domain with CPU and GPU hardware available from three mainstream vendors. The results are shown to exhibit good agreement with a post-event survey. Different configurations are evaluated for the program structure and data caching, with results demonstrating the new software’s suitability for use with different types of modern processing device. Performance scaling is similar to differences in quoted peak performance figures supplied by the vendors. We also compare results obtained with 32-bit and 64-bit floating-point computation, and find there are significant localised errors introduced by 32-bit precision.}
}

@article{ZHANG,
author = {Shanghong Zhang  and Rui Yuan  and Yu Wu  and Yujun Yi },
title = {Parallel Computation of a Dam-Break Flow Model Using OpenACC Applications},
journal = {Journal of Hydraulic Engineering},
volume = {143},
number = {1},
pages = {04016070},
year = {2017},
doi = {10.1061/(ASCE)HY.1943-7900.0001225},

URL = {https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29HY.1943-7900.0001225},
eprint = {https://ascelibrary.org/doi/pdf/10.1061/%28ASCE%29HY.1943-7900.0001225}
,
    abstract = { Two key factors in dam-break modeling are accuracy and speed. Therefore, high-performance calculations are of great importance to the simulation of dam-break events. In this study, we develop a two-dimensional hydrodynamic model based on the finite volume method to simulate the dam-break flow routing process. Roe’s approximate Riemann solution is adopted to solve the interface flux of grid cells and accurately simulate the discontinuous flow. A graphics processing unit (GPU)-based parallel method, OpenACC, is used to realize parallel computing. Because an explicit discrete technique is used to solve the governing equations, and there is no correlation between grid calculations in a single time step, the parallel dam-break model can be easily realized by adding OpenACC directives to the loop structure of the grid calculations. To analyze the performance of the model, we considered the Pangtoupao flood storage area in China using a Nvidia Tesla K20c card and four different grid division schemes. By carefully studying the implementation method and optimization of data transportation in the parallel algorithm, a speedup factor of 20.70 can be achieved. This acceleration is better than that of the OpenMP method with a 16-kernel computer. Further analysis reveals that models involving a larger number of calculations exhibit greater efficiency and a higher speedup rate. In addition, the OpenACC parallel mode has good portability, making it easy to realize parallel computation from the original serial model. This GPU-based parallel computation has the advantages of high performance and easily available required hardware. }
}

@article{LIU,
author = {Liu, Qiang and Qin, Yi and Li, Guodong},
TITLE = {Fast Simulation of Large-Scale Floods Based on GPU Parallel Computing},
JOURNAL = {Water},
VOLUME = {10},
YEAR = {2018},
NUMBER = {5},
ARTICLE-NUMBER = {589},
URL = {https://www.mdpi.com/2073-4441/10/5/589},
ISSN = {2073-4441},
ABSTRACT = {Computing speed is a significant issue of large-scale flood simulations for real-time response to disaster prevention and mitigation. Even today, most of the large-scale flood simulations are generally run on supercomputers due to the massive amounts of data and computations necessary. In this work, a two-dimensional shallow water model based on an unstructured Godunov-type finite volume scheme was proposed for flood simulation. To realize a fast simulation of large-scale floods on a personal computer, a Graphics Processing Unit (GPU)-based, high-performance computing method using the OpenACC application was adopted to parallelize the shallow water model. An unstructured data management method was presented to control the data transportation between the GPU and CPU (Central Processing Unit) with minimum overhead, and then both computation and data were offloaded from the CPU to the GPU, which exploited the computational capability of the GPU as much as possible. The parallel model was validated using various benchmarks and real-world case studies. The results demonstrate that speed-ups of up to one order of magnitude can be achieved in comparison with the serial model. The proposed parallel model provides a fast and reliable tool with which to quickly assess flood hazards in large-scale areas and, thus, has a bright application prospect for dynamic inundation risk identification and disaster assessment.},
DOI = {10.3390/w10050589}
}

@INPROCEEDINGS{Herdman,
  author={Herdman, J. A. and Gaudin, W. P. and McIntosh-Smith, S. and Boulton, M. and Beckingsale, D. A. and Mallinson, A. C. and Jarvis, S. A.},
  booktitle={2012 SC Companion: High Performance Computing, Networking Storage and Analysis}, 
  title={Accelerating Hydrocodes with OpenACC, OpenCL and CUDA}, 
  year={2012},
  volume={},
  number={},
  pages={465-471},
  keywords={Graphics processing units;Kernel;Programming;Hydrodynamics;Performance evaluation;Productivity;Libraries;OpenACC;OpenCL;CUDA;Hydrodynamics;High Performance Computing},
  doi={10.1109/SC.Companion.2012.66}}

@article{Hu2018,
author = {Hu, Xiaozhang and Song, Lixiang},
year = {2018},
month = {03},
pages = {},
title = {Hydrodynamic modeling of flash flood in mountain watersheds based on high-performance GPU computing},
volume = {91},
journal = {Natural Hazards},
doi = {10.1007/s11069-017-3141-7}
}

@article{MORALESHERNANDEZ2021105034,
title = {TRITON: A Multi-GPU open source 2D hydrodynamic flood model},
journal = {Environmental Modelling & Software},
volume = {141},
pages = {105034},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105034},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000773},
author = {M. Morales-Hernández and Md B. Sharif and A. Kalyanapu and S.K. Ghafoor and T.T. Dullo and S. Gangrade and S.-C. Kao and M.R. Norman and K.J. Evans},
keywords = {2D flood model, Open source, Multi-GPU, High-resolution, Shallow water equations},
abstract = {A new open source multi-GPU 2D flood model called TRITON is presented in this work. The model solves the 2D shallow water equations with source terms using a time-explicit first order upwind scheme based on an Augmented Roe's solver that incorporates a careful estimation of bed strengths and a local implicit formulation of friction terms. The scheme is demonstrated to be first order accurate, robust and able to solve for flows under various conditions. TRITON is implemented such that the model effectively utilizes heterogeneous architectures, from single to multiple CPUs and GPUs. Different test cases are shown to illustrate the capabilities and performance of the model, showing promising runtimes for large spatial and temporal scales when leveraging the computer power of GPUs. Under this hardware configuration, communication and input/output subroutines may impact the scalability. The code is developed under an open source license and can be freely downloaded in https://code.ornl.gov/hydro/triton.}
}

@article{SALEEM2024106141,
title = {Accelerated numerical modeling of shallow water flows with MPI, OpenACC, and GPUs},
journal = {Environmental Modelling & Software},
volume = {180},
pages = {106141},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106141},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224002020},
author = {Ayhan H. Saleem and Matthew R. Norman},
keywords = {Shallow water flows, Flood, Hydrodynamic modeling, Runge-Kutta, Unstructured grid, Domain decomposition, MPI, OpenACC},
abstract = {In this paper, a time-explicit Finite-Volume method is adopted to solve the 2-D shallow water equations on an unstructured triangular mesh, using a two-stage Runge-Kutta integrator and a monotone MUSCL model to achieve second-order accuracy in time and space, respectively. A multi-GPU model is presented that uses the Message Passing Interface (MPI) with OpenACC and uses the METIS library to produce the domain decomposition. A CUDA-aware MPI library (GPUDirect) and overlapped MPI communication with computation are used to improve parallel performance. Two benchmark tests with wet and dry downstream beds are used to test the code's accuracy. Good results were achieved compared to the numerical simulations of published studies. Compared with the multi-CPU version of a 6-core CPU, maximum speedups of 56.18 and 331.51 were obtained using a single GPU and 8 GPUs, respectively. Higher mesh resolution enhances acceleration performance, and the model is applicable to other environmental modeling activities.}
}

@inproceedings{Saetra2010,
author = {Sætra, Martin and Brodtkorb, André},
year = {2010},
month = {06},
pages = {56-66},
title = {Shallow Water Simulations on Multiple GPUs},
volume = {7134},
isbn = {978-3-642-28144-0},
journal = {Applied Parallel and Scientific Computing},
doi = {10.1007/978-3-642-28145-7_6}
}

@article{SERGHEI,
author = {Caviedes-Voullième, Daniel and Morales-Hernández, Mario and Norman, Matthew R and Özgen-Xian, Ilhan},
year = {2023},
month = {02},
pages = {977-1008},
title = {SERGHEI (SERGHEI-SWE) v1.0: a performance-portable high-performance parallel-computing shallow-water solver for hydrology and environmental hydraulics},
volume = {16},
journal = {Geoscientific Model Development},
doi = {10.5194/gmd-16-977-2023}
}

@phdthesis{Sandra,
  author = {Soares Frazão, Sandra},
  title = {Dam-break Induced Flows in Complex Topographies: Theoretical, Numerical and Experimental Approaches},
  school = {Université Catholique de Louvain},
  year = {2007},
  advisor = {Zech, Yves}
}

@manual{Watlab,
  title        = {Watlab},
  author       = {{Hydraulics Group, Université catholique de Louvain}},
  organization = {Université catholique de Louvain},
  note         = {Accessed: March 29, 2025},
  url          = {https://sites.uclouvain.be/hydraulics-group/watlab/}
}

@manual{pip,
  author    = {The Python Packaging Authority (PyPA)},
  title     = {pip - The Python Package Installer},
  year      = {2024},
  url       = {https://pip.pypa.io/},
  note      = {Accessed: 2024-03-29}
}

@book{lee2016introduction,
  title = {Introduction to Embedded Systems—A Cyber-Physical Systems Approach},
  author = {Lee, Edward A. and Seshia, Sanjit A.},
  edition = {2nd},
  year = {2016},
  publisher = {MIT Press},
  isbn = {978-0-262-53381-2}
}

@misc{openmp,
  author = {{OpenMP Architecture Review Board}},
  title = {{OpenMP} Application Programming Interface},
  year = {2023},
  note = {Version 5.2},
  url = {https://www.openmp.org/specifications/},
  urldate = {2024-06-20}
}

@book{MPI-standard,
  author    = {Message Passing Interface Forum},
  title     = {MPI: A Message-Passing Interface Standard, Version 4.0},
  year      = {2021},
  publisher = {University of Tennessee},
  note      = {\url{https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf}}
}

@techreport{metis,
  author = {Karypis, George and Kumar, Vipin},
  title = {{METIS}: A Software Package for Partitioning Unstructured Graphs, Partitioning Meshes, and Computing Fill-Reducing Orderings of Sparse Matrices},
  year = {1997},
  institution = {University of Minnesota},
  type = {Report},
  number = {97-061},
  url = {https://hdl.handle.net/11299/215346},
  urldate = {2024-06-20}
}

@article{COLINDEVERDIERE201178,
title = {Introduction to GPGPU, a hardware and software background},
journal = {Comptes Rendus Mécanique},
volume = {339},
number = {2},
pages = {78-89},
year = {2011},
note = {High Performance Computing},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1631072110002044},
author = {Guillaume {Colin de Verdière}},
keywords = {Computer science, GPGPU, Computer hardware, Programming languages, Informatique, GPGPU, Matériel, Logiciels},
abstract = {This article gives an introduction to GPU usage for High Performance Computing. After setting the context, we will describe the hardware and the programming languages currently available to programmers. From these explanations we will touch on the implications of these technologies for simulation codes and try to give trends for the future.
Résumé
Cet article est une introduction au calcul sur GPU pour le monde du Calcul Hautes Performances. Après avoir resitué le contexte, nous décrirons les matériels et les logiciels à la disposition des programmeurs. Sur la base de nos explications, nous esquisserons les implications de ces technologies sur les codes de calcul. Enfin, nous essayerons de dégager des tendances pour le futur.}
}

@article{ss,
author = {Oliveira, Suely and Stewart, David},
year = {2006},
month = {01},
pages = {},
title = {Writing Scientific Software: A Guide to Good Style},
isbn = {9780521858960},
journal = {Writing Scientific Software: A Guide for Good Style},
doi = {10.1017/CBO9780511617973}
}

@article{memory_wall,
author = {Wulf, Wm and McKee, Sally},
year = {1996},
month = {01},
pages = {},
title = {Hitting the Memory Wall: Implications of the Obvious},
volume = {23},
journal = {Computer Architecture News}
}

@inproceedings{RCM,
author = {Cuthill, E. and McKee, J.},
title = {Reducing the bandwidth of sparse symmetric matrices},
year = {1969},
isbn = {9781450374934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/800195.805928},
doi = {10.1145/800195.805928},
abstract = {The finite element displacement method of analyzing structures involves the solution of large systems of linear algebraic equations with sparse, structured, symmetric coefficient matrices. There is a direct correspondence between the structure of the coefficient matrix, called the stiffness matrix in this case, and the structure of the spatial network delineating the element layout. For the efficient solution of these systems of equations, it is desirable to have an automatic nodal numbering (or renumbering) scheme to ensure that the corresponding coefficient matrix will have a narrow bandwidth. This is the problem considered by R. Rosen1. A direct method of obtaining such a numbering scheme is presented. In addition several methods are reviewed and compared.},
booktitle = {Proceedings of the 1969 24th National Conference},
pages = {157-172},
numpages = {16},
series = {ACM '69}
}

@inproceedings{Valgrind,
  author    = {Nicholas Nethercote and Julian Seward},
  title     = {Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation},
  booktitle = {Proceedings of the 2007 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  year      = {2007},
  pages     = {89--100},
  publisher = {ACM},
  doi       = {10.1145/1250734.1250746},
  url       = {https://valgrind.org/},
}

@article{GMSH,
author = {Geuzaine, Christophe and Remacle, Jean-François},
year = {2009},
month = {09},
pages = {1309 - 1331},
title = {Gmsh: A 3-D Finite Element Mesh Generator with Built-in Pre- and Post-Processing Facilities},
volume = {79},
journal = {International Journal for Numerical Methods in Engineering},
doi = {10.1002/nme.2579}
}

@article{quicksort,
  title={Quicksort},
  author={Hoare, Charles AR},
  journal={The Computer Journal},
  volume={5},
  number={1},
  pages={10--16},
  year={1962},
  publisher={Oxford University Press}
}

@article{Testa2007,
  author = {Testa, Guido and Zuccalà, David and Alcrudo, Francisco and Mulet, Jontan and Soares-Frazão, Sandra},
  year = {2007},
  title = {Flash Flood Flow Experiment in a Simplified Urban District},
  journal = {Journal of Hydraulic Research},
  volume = {45},
  number = {sup1},
  pages = {37--44},
  doi = {10.1080/00221686.2007.9521831}
}

@manual{nvidia2024cuda,
  title        = {CUDA C++ Programming Guide},
  author       = {{NVIDIA Corporation}},
  year         = {2024},
  edition      = {Version 12.8},
  note         = {\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}},
  organization = {NVIDIA},
  type         = {Manual}
}

@article{cuda_diagram,
author = {Li, An and Maunder, Robert and Al-Hashimi, Bashir and Hanzo, L.},
year = {2016},
month = {06},
pages = {1-1},
title = {Implementation of a Fully-Parallel Turbo Decoder on a General-Purpose Graphics Processing Unit},
volume = {4},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2016.2586309}
}
