#import "@preview/subpar:0.2.2"

#text(fill: gradient.linear(red, blue))[
  It will be cool to prove that Watlab is memory bound and analyze data intensity.
]

From their inception, CPUs have continually become faster. Memory speeds have also increased, but not at the same pace as CPUs @ss. While a processor can perform several operations per clock cycle, accessing data from main memory can take tens of cycles @Eijkhout. This phenomenon is known as the memory wall @memory_wall.

To alleviate this bottleneck, faster on-chip memory called _cache_ was developed, greatly reducing memory access times. However, cache hardware consumes more power and requires more transistors than main memory, which limits its capacity. Caches are divided into _cache lines_, typically 64 bytes long on modern processors. Each cache line stores a copy of a memory block from main memory.

When the CPU needs to read an address from main memory, it first checks whether the data is already present in the cache. If it is, this is called a _cache hit_. If not, it results in a _cache miss_, and the data must be loaded from main memory into a cache line, possibly evicting another line to make room, before being moved from the cache to the registers.

Although cache behavior is managed by the hardware and not under direct programmer control, writing code with cache usage in mind can significantly improve performance. Specifically, maximizing _temporal_ and _spatial locality_ increases the likelihood of cache hits. Temporal locality refers to the tendency of programs to reuse the same data within short time intervals. Spatial locality means that programs tend to access memory locations that are close to each other @Eijkhout.

To evaluate the number of cache hits and misses during the execution of Watlab, we used Cachegrind, a tool from the Valgrind framework @Valgrind, which simulates cache behavior and provides line-by-line annotations of source code indicating the number of cache read and write misses.

In the original Watlab implementation, intermediate results used in the flux computations at boundary and inner interfaces were stored in member variables—i.e., variables that are part of a class object and accessible by all functions within the class. Cachegrind analysis revealed that these member variables caused numerous cache misses, as they were accessed only once per iteration and stored in main memory. Furthermore, most of these variables were only used inside the flux computation function.

As a first memory optimization, we converted these into local variables, restricting their scope to the flux computation function whenever possible. This ensured they were stored on the stack, which slightly reduced execution times as shown in [REF].

Additionally, other cache misses were related to memory accesses during iterations over cells—for computing source terms, updating hydraulic variables, computing the next time step—or over interfaces for flux computation. These accesses involve simple traversals of arrays of structures (cells and interfaces), and cannot be further optimized. The memory layout of cells, nodes, and interfaces follows the order in the input files generated by the preprocessing Python API.

However, during flux computations at each interface, we must access the left and right cells to retrieve associated water heights for inner interfaces, or only the left cell for boundary interfaces. If the cells are renumbered such that the index difference between left and right cells is minimized, we can benefit from improved spatial locality, as the accessed cells are likely to be closer in memory. \
Fortunately, this is precisely what the reverse Cuthill-McKee algorithm (RCM) can help us to do. It is initially designed to permute a symmetric sparse matrix in order to reduce its bandwidth. In our case, this matrix corresponds to the adjacency matrix derived from the dual graph associated with the mesh. If you consider each cell as a node in a graph connected to neighboring cells, i.e., each inner interface represents an edge, you can easily derive an adjacency matrix. \
To illustrate, we consider a simple square mesh composed of 26 triangular elements  (@square_mesh). The lower triangular part of the original symmetric adjacency matrix is shown in @adj_v1, while the upper triangular part represents the updated matrix after applying the reverse Cuthill-McKee algorithm. The bandwidth of the resulting matrix is much smaller, meaning that left and right cell indices of each interface are now closer in memory. \
However, REF shows that these reordered meshes do not lead to reduced execution times. This is because, while spatial locality is improved, temporal locality is simultaneously degraded. The colors of each entry in the adjacency matrices represent the corresponding interface indices, ranging from blue (low indices) to rose (high indices). In the initial lower matrix, we observe a smooth gradient from left to right. This is linked to how GMSH @GMSH, the mesh generator used to produce the mesh files feeding Watlab, numbers the interfaces. A closer look at @square_mesh reveals that it first numbers boundary interfaces in a counterclockwise manner. Then, inner interfaces are ordered by the left cell index and, for interfaces sharing the same left index, by the right cell index. As interfaces are processed in order, this improves temporal locality by increasing the likelihood of accessing common neighboring cells consecutively. \
After reordering the cells, we lose this benefit: the new indices disrupt the original sorting, as shown by the shuffled colors in the updated adjacency matrix of @adj_v1. The solution, however, is quite straightforward: we can simply renumber the interfaces by sorting them based on the new left and right indices, using an algorithm like Quicksort @quicksort (@adj_v2). \
The obtained timings are reported in REF.

#subpar.grid(
  figure(image("../img/no_RCM.svg", width: 81%), gap: .75em, caption: [
    Before reordering
  ]), <square_mesh>,
  figure(image("../img/RCM.svg", width: 81%), gap: .75em, caption: [
    After reordering
  ]), <after_reordering>,
  columns: 1,
  caption: [A simple square mesh with 26 cells and 45 interfaces],
  placement: auto,
  numbering: n => {
    let h1 = counter(heading).get().first()
    numbering("1.1", h1, n)
  }, gap: 1em
)

#subpar.grid(
  figure(move(image("../img/adj_RCM.svg", width: 77%), dx:6%), gap: .75em, caption: [
    Renumbering cells using the reverse Cuthill-McKee algorithm
  ]), <adj_v1>,
  figure(move(image("../img/adj_RCM_QS.svg", width: 77%), dx: 6%), gap: .75em, caption: [
    Renumbering cells using the reverse Cuthill-McKee algorithm and interfaces using Quicksort
  ]), <adj_v2>,
  columns: 1,
  caption: [Adjacency matrices related to the square mesh],
  placement: auto,
  numbering: n => {
    let h1 = counter(heading).get().first()
    numbering("1.1", h1, n)
  }, gap: 1em
)


dire pourquoi j'ai mis le reordering dans python

tester si cest ap mieux de faire comme GMSH pour numeroter les edges
i.e. d'abord numeroter les frontieres puis les inners





