Decoding and processing instructions is an expensive operation in terms of time, energy, and the number of required transistors. Graphics Processing Units (GPUs) were developed from the idea of reducing the functionality of parallel cores to save on each of these costs, thereby enabling a much higher number of cores. These massively parallel architectures were originally designed to offload from the CPU the task of rendering two-dimensional images from a three-dimensional virtual world @Brodtkorb2013, by processing each pixel in parallel.

The functional equivalent of CPU cores in GPUs are called _Streaming Multiprocessors_ (SMs). Each SM contains execution cores—both single- and double-precision floating-point units—as well as special function units, schedulers, caches, and registers. A sequence of instructions is abstracted as _threads_, which are grouped into _warps_ that execute concurrently and truly simultaneously in lockstep on the SMs. Consequently, execution on GPUs follows the Single Instruction, Multiple Threads (SIMT) paradigm.

The growing number of numerical simulations exhibiting massive data parallelism in both industry and academia has made the use of GPUs for non-graphics applications increasingly popular @COLINDEVERDIERE201178. This trend is known as General-Purpose computing on GPUs (GPGPU). However, since GPUs are fundamentally different hardware with their own architectural paradigms, they also require different programming languages than those commonly used for CPU programming.